use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct CompletionUsage {
    /// Number of tokens in the generated completion.
    pub completion_tokens: Option<u32>,

    /// Number of tokens in the prompt.
    pub prompt_tokens: Option<u32>,

    /// Total number of tokens used in the request (prompt + completion).
    pub total_tokens: Option<u32>,

    /// Breakdown of tokens used in a completion.
    pub completion_tokens_details: Option<CompletionTokensDetails>,

    /// Breakdown of tokens used in the prompt.
    pub prompt_tokens_details: Option<PromptTokensDetails>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct CompletionTokensDetails {
    /// When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion.
    pub accepted_prediction_tokens: Option<u32>,

    /// Audio input tokens generated by the model.
    pub audio_tokens: Option<u32>,

    /// Tokens generated by the model for reasoning.
    pub reasoning_tokens: Option<u32>,

    /// When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits.
    pub rejected_prediction_tokens: Option<u32>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct PromptTokensDetails {
    /// Audio input tokens present in the prompt.
    pub audio_tokens: Option<u32>,

    /// Cached tokens present in the prompt.
    pub cached_tokens: Option<u32>,
}
